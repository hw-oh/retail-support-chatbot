import json
import weave
import asyncio
from typing import Dict, List, Any
from simple_chatbot import SimplifiedChatbot
# ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜ë“¤ì€ get_scoring_functions()ì—ì„œ ë™ì ìœ¼ë¡œ import

class RefundChatbotModel(weave.Model):
    """í™˜ë¶ˆ ì±—ë´‡ í‰ê°€ë¥¼ ìœ„í•œ Weave Model í´ë˜ìŠ¤ (ë‹¨ì¼ ì±— ì‚¬ìš©)"""
    
    @weave.op()
    def predict(self, user_query: str) -> Dict[str, Any]:
        """
        ì‚¬ìš©ì ì¿¼ë¦¬ì— ëŒ€í•œ ì±—ë´‡ ì‘ë‹µì„ ì˜ˆì¸¡ (ë‹¨ì¼ ì±— ë°©ì‹)
        
        Args:
            user_query: ì‚¬ìš©ì ì§ˆë¬¸
        
        Returns:
            ì±—ë´‡ ì‘ë‹µ
        """
        try:
            # ìƒˆë¡œìš´ ì±—ë´‡ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë‹¨ì¼ ëŒ€í™”
            chatbot = SimplifiedChatbot()
            response = chatbot.chat(user_query)
            
            # ì±—ë´‡ì˜ conversation_contextì—ì„œ agent_data ì¶”ì¶œ
            agent_data = None
            if chatbot.conversation_context:
                last_turn = chatbot.conversation_context[-1]
                agent_data = last_turn.get('agent_data')
            
            result = {
                "response": response,
                "user_query": user_query
            }
            
            # agent_dataê°€ ìˆìœ¼ë©´ í¬í•¨
            if agent_data:
                result["agent_data"] = agent_data
            
            return result
            
        except Exception as e:
            return {
                "response": f"Error: {str(e)}",
                "user_query": user_query,
                "error": str(e)
            }

def load_evaluation_dataset() -> List[Dict]:
    """í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ ë° Weave í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    with open('data/evaluate_refund.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    examples = []
    for item in data:
        example = {
            "id": item["test_id"],
            "user_query": item["user_query"],
            "scenario": item["scenario"],
            "order_info": item["order_info"],
            "expected_result": item["expected_result"]
        }
        examples.append(example)
    
    return examples

def get_scoring_functions():
    """ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜ë“¤ì„ ë°˜í™˜ (Weaveì— ìë™ ë“±ë¡ë¨)"""
    print("ğŸ“ ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜ë“¤ ì¤€ë¹„ ì¤‘...")
    
    # ìƒˆë¡œìš´ ê°„ë‹¨í•œ ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜ë“¤ (ì´ë¯¸ @weave.op()ë¡œ ë“±ë¡ë¨)
    from scorers import (
        exact_match_refund_decision,
        llm_judge_reason_quality,
        exact_match_refund_fee,
        llm_judge_policy_compliance
    )
    
    scorers = [
        exact_match_refund_decision,
        llm_judge_reason_quality,
        exact_match_refund_fee,
        llm_judge_policy_compliance
    ]
    
    print("âœ… ëª¨ë“  ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ")
    return scorers

async def run_comprehensive_evaluation():
    """ì „ì²´ í‰ê°€ ì‹œìŠ¤í…œ ì‹¤í–‰"""
    # Weave ì´ˆê¸°í™”
    weave.init('retail-chatbot-dev')
    
    # ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜ë“¤ ê°€ì ¸ì˜¤ê¸°
    scorers = get_scoring_functions()
    
    # ëª¨ë¸ ìƒì„±
    model = RefundChatbotModel()
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    examples = load_evaluation_dataset()
    print(f"ğŸ“Š {len(examples)}ê°œì˜ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œë¨")
    
    # í‰ê°€ ì„¤ì •
    evaluation = weave.Evaluation(
        name="comprehensive_refund_evaluation",
        dataset=examples,
        scorers=scorers
    )
    
    print("ğŸš€ ì¢…í•© í™˜ë¶ˆ ì±—ë´‡ í‰ê°€ ì‹œì‘...")
    
    # í‰ê°€ ì‹¤í–‰
    results = await evaluation.evaluate(model)
    
    print("âœ… í‰ê°€ ì™„ë£Œ!")
    print(f"ğŸ“ˆ ê²°ê³¼ ìš”ì•½:")
    
    # ê²°ê³¼ ë¶„ì„
    if hasattr(results, 'summary'):
        for scorer_name, score in results.summary.items():
            print(f"  - {scorer_name}: {score:.3f}")
    
    return results

async def run_sample_evaluation(sample_size: int = 5):
    """ìƒ˜í”Œ í‰ê°€ ì‹¤í–‰ (ê°œë°œ/í…ŒìŠ¤íŠ¸ìš©)"""
    # Weave ì´ˆê¸°í™”
    weave.init('retail-chatbot-dev')
    
    # ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜ë“¤ ê°€ì ¸ì˜¤ê¸°
    scorer_functions = get_scoring_functions()
    
    # ëª¨ë¸ ìƒì„±
    model = RefundChatbotModel()
    
    # ìƒ˜í”Œ ë°ì´í„°
    all_examples = load_evaluation_dataset()
    examples = all_examples[:sample_size]
    
    print(f"ğŸ“Š {len(examples)}ê°œì˜ ìƒ˜í”Œ ì‹œë‚˜ë¦¬ì˜¤ë¡œ í…ŒìŠ¤íŠ¸...")
    
    results = []
    
    for i, example in enumerate(examples, 1):
        print(f"\nğŸ§ª [{i}/{len(examples)}] {example['id']} - {example['scenario']}")
        print(f"ğŸ‘¤ ì§ˆë¬¸: {example['user_query']}")
        
        # ëª¨ë¸ ì˜ˆì¸¡
        prediction = model.predict(example['user_query'])
        print(f"ğŸ¤– ì‘ë‹µ: {prediction['response'][:100]}...")
        
        # ê° ìŠ¤ì½”ì–´ëŸ¬ë¡œ í‰ê°€
        scores = {}
        scorer_names = ['exact_match_refund_decision', 'llm_judge_reason_quality', 'exact_match_refund_fee', 'llm_judge_policy_compliance']
        
        for i, scorer_func in enumerate(scorer_functions):
            scorer_name = scorer_names[i]
            try:
                score_result = scorer_func(example['expected_result'], prediction)
                scores[scorer_name] = score_result
                # print(f"DEBUG {scorer_name} ê²°ê³¼: {score_result}")
            except Exception as e:
                print(f"âš ï¸ {scorer_name} í‰ê°€ ì‹¤íŒ¨: {e}")
                scores[scorer_name] = {"error": str(e)}
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"ğŸ“Š í‰ê°€ ê²°ê³¼:")
        for scorer_name, score_data in scores.items():
            if isinstance(score_data, dict) and "error" not in score_data:
                main_score = None
                # ìƒˆë¡œìš´ ìŠ¤ì½”ì–´ë§ í•¨ìˆ˜ì˜ í‚¤ì— ë§ì¶° ì¡°ì •
                if "refund_decision_accuracy" in score_data:
                    main_score = score_data["refund_decision_accuracy"]
                elif "reason_quality_score" in score_data:
                    main_score = score_data["reason_quality_score"]
                elif "refund_fee_accuracy" in score_data:
                    main_score = score_data["refund_fee_accuracy"]
                elif "policy_compliance_score" in score_data:
                    main_score = score_data["policy_compliance_score"]
                
                if main_score is not None:
                    print(f"  - {scorer_name}: {main_score:.3f}")
                else:
                    print(f"  - {scorer_name}: {score_data}")
            elif isinstance(score_data, dict) and "error" in score_data:
                print(f"  - {scorer_name}: ERROR ({score_data['error']})")
            else:
                print(f"  - {scorer_name}: {score_data}")
        
        results.append({
            "example": example,
            "prediction": prediction,
            "scores": scores
        })
    
    # ì „ì²´ ìš”ì•½
    print(f"\nğŸ“ˆ ìƒ˜í”Œ í‰ê°€ ì™„ë£Œ! ({len(results)}ê°œ ì‹œë‚˜ë¦¬ì˜¤)")
    
    # í‰ê·  ì ìˆ˜ ê³„ì‚°
    scorer_averages = {}
    scorer_names = ['exact_match_refund_decision', 'llm_judge_reason_quality', 'exact_match_refund_fee', 'llm_judge_policy_compliance']
    for scorer_name in scorer_names:
        valid_scores = []
        for result in results:
            score_data = result["scores"].get(scorer_name, {})
            if isinstance(score_data, dict) and "error" not in score_data:
                if "refund_decision_accuracy" in score_data:
                    valid_scores.append(score_data["refund_decision_accuracy"])
                elif "reason_quality_score" in score_data:
                    valid_scores.append(score_data["reason_quality_score"])
                elif "refund_fee_accuracy" in score_data:
                    valid_scores.append(score_data["refund_fee_accuracy"])
                elif "policy_compliance_score" in score_data:
                    valid_scores.append(score_data["policy_compliance_score"])
        
        if valid_scores:
            scorer_averages[scorer_name] = sum(valid_scores) / len(valid_scores)
        else:
            scorer_averages[scorer_name] = 0.0
    
    print("\nğŸ“Š í‰ê·  ì ìˆ˜:")
    for scorer_name, avg_score in scorer_averages.items():
        print(f"  - {scorer_name}: {avg_score:.3f}")
    
    return results

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ¯ í™˜ë¶ˆ ì±—ë´‡ í‰ê°€ ì‹œìŠ¤í…œ")
    print("1. ìƒ˜í”Œ í‰ê°€ (5ê°œ)")
    print("2. ì „ì²´ í‰ê°€ (30ê°œ)")
    
    choice = input("ì„ íƒí•˜ì„¸ìš” (1 ë˜ëŠ” 2): ").strip()
    
    if choice == "1":
        await run_sample_evaluation(5)
    elif choice == "2":
        await run_comprehensive_evaluation()
    else:
        print("ìƒ˜í”Œ í‰ê°€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
        await run_sample_evaluation(5)

if __name__ == "__main__":
    asyncio.run(main())
