import json
import weave
import asyncio
from evaluate_chatbot import RefundChatbotModel, refund_decision_accuracy, reason_quality_score, refund_fee_accuracy, policy_compliance_score

async def test_small_evaluation():
    # Weave ì´ˆê¸°í™”
    weave.init('retail-chatbot-dev')
    
    # ëª¨ë¸ ìƒì„±
    model = RefundChatbotModel()
    
    # ì‘ì€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ (ì²˜ìŒ 3ê°œ)
    with open('data/evaluate_refund.json', 'r', encoding='utf-8') as f:
        all_data = json.load(f)
    
    # ì²˜ìŒ 3ê°œë§Œ í…ŒìŠ¤íŠ¸
    test_data = all_data[:3]
    
    examples = []
    for item in test_data:
        example = {
            "id": item["test_id"],
            "user_query": item["user_query"],
            "order_info": item["order_info"],
            "scenario": item["scenario"],
            "target": item["expected_result"]
        }
        examples.append(example)
    
    print(f"ğŸ“Š {len(examples)}ê°œì˜ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ë¡œ ì‹œì‘...")
    
    # ê°œë³„ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    for example in examples:
        print(f"\nğŸ§ª í…ŒìŠ¤íŠ¸: {example['id']} - {example['scenario']}")
        print(f"ğŸ‘¤ ì§ˆë¬¸: {example['user_query']}")
        
        # ëª¨ë¸ ì˜ˆì¸¡
        result = model.predict(example['user_query'], example['order_info'])
        print(f"ğŸ¤– ì‘ë‹µ: {result['response'][:100]}...")
        
        # ê° í‰ê°€ í•¨ìˆ˜ ì‹¤í–‰
        accuracy = refund_decision_accuracy(example['target'], result)
        reason = reason_quality_score(example['target'], result)
        fee = refund_fee_accuracy(example['target'], result)
        compliance = policy_compliance_score(example['target'], result)
        
        print(f"ğŸ“Š í‰ê°€ ê²°ê³¼:")
        print(f"  - í™˜ë¶ˆ ì—¬ë¶€ ì •í™•ë„: {accuracy['accuracy']:.2f}")
        print(f"  - ì´ìœ  ì„¤ëª… í’ˆì§ˆ: {reason['reason_score']:.2f}")
        print(f"  - ìˆ˜ìˆ˜ë£Œ ì •í™•ë„: {fee['fee_accuracy']:.2f}")
        print(f"  - ì •ì±… ì¤€ìˆ˜ë„: {compliance['policy_compliance']:.2f}")
    
    print("\nâœ… ì‘ì€ ê·œëª¨ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    asyncio.run(test_small_evaluation())
