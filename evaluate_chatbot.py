import json
import weave
import asyncio
from typing import Dict, List, Any
from simple_chatbot import SimplifiedChatbot
from scorers.policy_compliance_scorer import PolicyComplianceScorer
from scorers.reason_quality_scorer import ReasonQualityScorer
from scorers.refund_decision_scorer import RefundDecisionScorer
from config import config

class RefundChatbotModel(weave.Model):
    """Refund chatbot evaluation Weave Model class with multi-language support"""
    
    language: str = "ko"  # Default language
    
    @weave.op()
    def predict(self, user_query: str, order_info: Dict = None, language: str = None) -> Dict[str, Any]:
        """
        Predict chatbot response to user query
        
        Args:
            user_query: User question
            order_info: Order information (optional)
            language: Language for evaluation (ko, en, jp)
        
        Returns:
            Chatbot response and analysis results
        """
        eval_language = language or self.language
        
        try:
            # Create chatbot instance with specified language and generate response
            chatbot = SimplifiedChatbot(language=eval_language)
            response = chatbot.chat(user_query, order_info)
            
            return {
                "response": response,
                "raw_response": response,
                "language": eval_language
            }
        except Exception as e:
            return {
                "response": f"Error: {str(e)}",
                "raw_response": f"Error: {str(e)}",
                "language": eval_language
            }

# Multi-language evaluation functions
@weave.op()
def policy_compliance_evaluation(target: Dict, output: Dict) -> Dict[str, Any]:
    """Policy compliance evaluation - LLM-based scoring with reason"""
    scorer = PolicyComplianceScorer()
    language = output.get("language", "ko")
    result = scorer.score(target, output, language)
    return {
        "accuracy": result.get("policy_compliance", 0.0),
        "reason": result.get("reason", "Evaluation failed")
    }

@weave.op()
def reasoning_performance_evaluation(target: Dict, output: Dict) -> Dict[str, Any]:
    """Reasoning performance evaluation - LLM-based scoring with reason"""
    scorer = ReasonQualityScorer()
    language = output.get("language", "ko")
    result = scorer.score(target, output, language)
    return {
        "accuracy": result.get("reason_score", 0.0),
        "reason": result.get("reason", "Evaluation failed")
    }

@weave.op()
def refund_accuracy_evaluation(target: Dict, output: Dict) -> Dict[str, Any]:
    """Refund accuracy evaluation - LLM-based evaluation (refund decision accuracy only)"""
    scorer = RefundDecisionScorer()
    language = output.get("language", "ko")
    result = scorer.score(target, output, language)
    return {
        "accuracy": result.get("accuracy", 0.0),  # Refund eligibility accuracy
        "reason": result.get("reason", "No evaluation result")
    }

def load_evaluation_dataset(language: str = "ko"):
    """Load evaluation dataset with language support"""
    try:
        # Load language-specific evaluation data
        data_path = config.get_data_path('evaluate_refund.json', language)
        with open(data_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
    except FileNotFoundError:
        # Fallback to Korean data
        with open('data/ko/evaluate_refund.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
    
    # Extract test_cases array from new JSON structure
    test_cases = data.get("test_cases", [])
    
    # Convert to Weave Evaluation format
    examples = []
    for item in test_cases:
        example = {
            "id": item["test_id"],
            "user_query": item["user_query"],
            "order_info": item["order_info"],
            "scenario": item["scenario"],
            "target": item["expected_result"],
            "language": language  # Add language information
        }
        examples.append(example)
    
    return examples

async def main(language: str = "ko"):
    """Main evaluation function with language support"""
    # Initialize Weave
    weave.init('retail-chatbot-dev')
    
    # Create model with specified language
    model = RefundChatbotModel(language=language)
    
    # Load dataset for the specified language
    examples = load_evaluation_dataset(language)
    
    language_names = {"ko": "í•œêµ­ì–´", "en": "English", "jp": "æ—¥æœ¬èª"}
    lang_name = language_names.get(language, language)
    
    print(f"ğŸ“Š {len(examples)} test scenarios loaded for {lang_name} ({language.upper()})")
    
    # Evaluation configuration - 3 core evaluations
    evaluation = weave.Evaluation(
        name=f"refund_chatbot_{language}_evaluation",
        dataset=examples,
        scorers=[
            policy_compliance_evaluation,      # Policy compliance
            reasoning_performance_evaluation,  # Reasoning performance
            refund_accuracy_evaluation        # Refund accuracy
        ]
    )
    
    if language == "ko":
        print("ğŸš€ í™˜ë¶ˆ ì±—ë´‡ í‰ê°€ ì‹œì‘...")
        print("ğŸ“‹ í‰ê°€ í•­ëª©:")
        print("   1. ì •ì±… ì¤€ìˆ˜ (Policy Compliance) - LLM ê¸°ë°˜ í‰ê°€ (ì ìˆ˜ + ì´ìœ )")
        print("   2. ì¶”ë¡  ì„±ëŠ¥ (Reasoning Performance) - LLM ê¸°ë°˜ í‰ê°€ (ì ìˆ˜ + ì´ìœ )")
        print("   3. í™˜ë¶ˆ ì •í™•ë„ (Refund Accuracy) - LLM ê¸°ë°˜ í‰ê°€ (ì ìˆ˜ + ì´ìœ )")
    elif language == "en":
        print("ğŸš€ Starting refund chatbot evaluation...")
        print("ğŸ“‹ Evaluation items:")
        print("   1. Policy Compliance - LLM-based evaluation (score + reason)")
        print("   2. Reasoning Performance - LLM-based evaluation (score + reason)")
        print("   3. Refund Accuracy - LLM-based evaluation (score + reason)")
    elif language == "jp":
        print("ğŸš€ è¿”å“ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆè©•ä¾¡ã‚’é–‹å§‹...")
        print("ğŸ“‹ è©•ä¾¡é …ç›®:")
        print("   1. ãƒãƒªã‚·ãƒ¼éµå®ˆ (Policy Compliance) - LLMãƒ™ãƒ¼ã‚¹è©•ä¾¡ (ã‚¹ã‚³ã‚¢ + ç†ç”±)")
        print("   2. æ¨è«–æ€§èƒ½ (Reasoning Performance) - LLMãƒ™ãƒ¼ã‚¹è©•ä¾¡ (ã‚¹ã‚³ã‚¢ + ç†ç”±)")
        print("   3. è¿”å“ç²¾åº¦ (Refund Accuracy) - LLMãƒ™ãƒ¼ã‚¹è©•ä¾¡ (ã‚¹ã‚³ã‚¢ + ç†ç”±)")
    
    # Execute evaluation
    results = await evaluation.evaluate(model)
    
    if language == "ko":
        print("\nâœ… í‰ê°€ ì™„ë£Œ!")
    elif language == "en":
        print("\nâœ… Evaluation completed!")
    elif language == "jp":
        print("\nâœ… è©•ä¾¡å®Œäº†!")
        
    print(f"ğŸ“ˆ Results: {results}")
    
    return results

async def evaluate_all_languages():
    """Evaluate chatbot for all supported languages"""
    print("ğŸŒ Multi-language Chatbot Evaluation")
    print("=" * 60)
    
    languages = ["ko", "en", "jp"]
    all_results = {}
    
    for lang in languages:
        print(f"\nğŸ”„ Evaluating {lang.upper()} chatbot...")
        try:
            result = await main(lang)
            all_results[lang] = result
            print(f"âœ… {lang.upper()} evaluation completed")
        except Exception as e:
            print(f"âŒ {lang.upper()} evaluation failed: {e}")
            all_results[lang] = {"error": str(e)}
    
    print("\n" + "=" * 60)
    print("ğŸ“Š Multi-language Evaluation Summary")
    print("=" * 60)
    
    for lang, result in all_results.items():
        if "error" in result:
            print(f"{lang.upper()}: âŒ Failed - {result['error']}")
        else:
            print(f"{lang.upper()}: âœ… Completed")
    
    return all_results
    

# Interactive language selection
def _prompt_language_selection() -> str:
    """Prompt user to select a language (or all) interactively."""
    language_names = {"ko": "í•œêµ­ì–´", "en": "English", "jp": "æ—¥æœ¬èª"}
    supported = list(config.SUPPORTED_LANGUAGES)
    options = supported + ["all"]

    print("\nğŸŒ ë‹¤êµ­ì–´ í‰ê°€ë¥¼ ìœ„í•œ ì–¸ì–´ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
    for idx, code in enumerate(options, start=1):
        if code == "all":
            print(f"  {idx}. ì „ì²´ ì–¸ì–´ í‰ê°€ (ALL)")
        else:
            print(f"  {idx}. {language_names.get(code, code)} ({code.upper()})")

    print("  0. ì¢…ë£Œ (Exit)")

    while True:
        choice = input("\në²ˆí˜¸ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸: 1): ").strip()
        if choice == "":
            return supported[0]  # default first supported (usually 'ko')
        if choice.isdigit():
            num = int(choice)
            if num == 0:
                return ""  # signal exit
            if 1 <= num <= len(options):
                return options[num - 1]
        # Also allow direct code input like 'ko', 'en', 'jp', or 'all'
        lowered = choice.lower()
        if lowered in options:
            return lowered
        print("â—ï¸ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”.")


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1].lower() == "all":
            # Evaluate all languages
            asyncio.run(evaluate_all_languages())
        else:
            # Evaluate specific language
            language = sys.argv[1].lower()
            if language in config.SUPPORTED_LANGUAGES:
                asyncio.run(main(language))
            else:
                print(f"âŒ Unsupported language: {language}")
                print(f"Supported languages: {', '.join(config.SUPPORTED_LANGUAGES)}")
    else:
        # Interactive language selection when no args provided
        selected = _prompt_language_selection()
        if selected == "":
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        elif selected == "all":
            asyncio.run(evaluate_all_languages())
        else:
            asyncio.run(main(selected))